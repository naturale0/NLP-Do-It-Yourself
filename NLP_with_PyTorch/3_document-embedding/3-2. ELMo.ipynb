{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "manufactured-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-painting",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>3-2. ELMo<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Prepare-dataset:-Gutenberg\" data-toc-modified-id=\"Prepare-dataset:-Gutenberg-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Prepare dataset: Gutenberg</a></span><ul class=\"toc-item\"><li><span><a href=\"#For-pretraining\" data-toc-modified-id=\"For-pretraining-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>For pretraining</a></span></li><li><span><a href=\"#For-training\" data-toc-modified-id=\"For-training-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>For training</a></span></li></ul></li><li><span><a href=\"#Build-the-model\" data-toc-modified-id=\"Build-the-model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Build the model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bidirectional-language-model\" data-toc-modified-id=\"Bidirectional-language-model-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Bidirectional language model</a></span></li><li><span><a href=\"#ELMo\" data-toc-modified-id=\"ELMo-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>ELMo</a></span></li></ul></li><li><span><a href=\"#Train-the-model\" data-toc-modified-id=\"Train-the-model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Train the model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pretrain-bidirectional-language-model\" data-toc-modified-id=\"Pretrain-bidirectional-language-model-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Pretrain bidirectional language model</a></span></li><li><span><a href=\"#Train-ELMo-for-sentiment-analysis\" data-toc-modified-id=\"Train-ELMo-for-sentiment-analysis-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Train ELMo for sentiment analysis</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-siemens",
   "metadata": {},
   "source": [
    "## Prepare dataset: Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "prepared-means",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/psh/.pyenv/versions/3.8.7/lib/python3.8/site-packages/thinc/neural/_custom_kernels.py:36: ResourceWarning: unclosed file <_io.TextIOWrapper name='/home/psh/.pyenv/versions/3.8.7/lib/python3.8/site-packages/thinc/neural/_custom_kernels.cu' mode='r' encoding='utf8'>\n",
      "  SRC = (PWD / \"_custom_kernels.cu\").open(\"r\", encoding=\"utf8\").read()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/home/psh/.pyenv/versions/3.8.7/lib/python3.8/site-packages/thinc/neural/_custom_kernels.py:39: ResourceWarning: unclosed file <_io.TextIOWrapper name='/home/psh/.pyenv/versions/3.8.7/lib/python3.8/site-packages/thinc/neural/_murmur3.cu' mode='r' encoding='utf8'>\n",
      "  MMH_SRC = (PWD / \"_murmur3.cu\").open(\"r\", encoding=\"utf8\").read()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "100%|██████████| 25000/25000 [00:26<00:00, 936.68lines/s] \n"
     ]
    }
   ],
   "source": [
    "from torchtext.experimental.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"spacy\")\n",
    "train, test = IMDB(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "taken-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = train.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "attractive-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = 2\n",
    "x = []\n",
    "y = []\n",
    "for _, words in train:\n",
    "    for i in range(len(words)-ngrams):\n",
    "        text = words[i:i+ngrams]\n",
    "        label = words[i+ngrams]\n",
    "        x.append(text.tolist())\n",
    "        y.append(label.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-clinton",
   "metadata": {},
   "source": [
    "```python\n",
    "with open(\"./IMBD_bigram.json\", \"w\") as w:\n",
    "    json.dump({\"data\": x, \"label\": y}, w)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-jersey",
   "metadata": {},
   "source": [
    "### For pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-dating",
   "metadata": {},
   "source": [
    "* transform to character-level n-gram dataset\n",
    "    * original script\n",
    "```\n",
    "%load https://gist.githubusercontent.com/akurniawan/30719686669dced49e7ced720329a616/raw/7b9f9967c01ce87ac505520a5aa58d3b24c55c66/translation_char_example.py\n",
    "```\n",
    "    * modified\n",
    "```\n",
    "%load https://gist.github.com/naturale0/6bb3b8a5c682bd281de87e408fa71bf1/raw/df8b7e198f149f81c4f72af977760b2eb3226cdf/translation_char_example.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "seasonal-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified a little to fit classification\n",
    "import itertools\n",
    "from torchtext.experimental.datasets import TextClassificationDataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.experimental.functional import sequential_transforms\n",
    "\n",
    "def build_char_vocab(data, index, bow=\"<w>\", eow=\"</w>\"):\n",
    "    \"\"\"\n",
    "    build character level vocabulary\n",
    "    \"\"\"\n",
    "    tok_list = [\n",
    "        [bow],\n",
    "        [eow],\n",
    "    ]\n",
    "    for line in data:\n",
    "        tokens = list(itertools.chain.from_iterable(line[index]))\n",
    "        tok_list.append(tokens)\n",
    "    return build_vocab_from_iterator(tok_list)\n",
    "\n",
    "\n",
    "def stoi(vocab):\n",
    "    \"\"\"\n",
    "    change string to index\n",
    "    \"\"\"\n",
    "    def func(tok_iter):\n",
    "        return [[vocab[char] for char in word]\\\n",
    "                for word in tok_iter]\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def tokenize_char(bow=\"<w>\", eow=\"</w>\", max_word_length=20):\n",
    "    \"\"\"\n",
    "    attach bow, eow token and pad with token\n",
    "    \"\"\"\n",
    "    def func(tok_iter):\n",
    "        result = np.empty((max(len_seq), max_word_length+2), dtype=object)\n",
    "        \n",
    "        # \"≥\" for padding\n",
    "        result[:len(tok_iter)] = [\n",
    "            [bow] + word + [eow] \\\n",
    "            + [\"<pad>\"] * (max_word_length - len(word)) \\\n",
    "            if len(word) < max_word_length \\\n",
    "            else [bow] + word[:max_word_length] + [eow]\n",
    "        for word in tok_iter]\n",
    "        \n",
    "        return result\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "desirable-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache training data for vocabulary construction\n",
    "train_data = [(line[0], [vocab.itos[ix] for ix in line[1]]) for line in zip(y, x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "noticed-netscape",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, ['I', 'rented']), (14567, ['rented', 'I']), (36197, ['I', 'AM'])]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "drawn-mainstream",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6760717lines [00:06, 993084.93lines/s]\n"
     ]
    }
   ],
   "source": [
    "# Setup vocabularies (both words and chars)\n",
    "char_vocab = build_char_vocab(train_data, index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "multiple-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the dataset with character level tokenization\n",
    "def char_tokenizer(words):\n",
    "    return [list(word) for word in words]\n",
    "\n",
    "char_transform = sequential_transforms(\n",
    "    char_tokenizer, \n",
    "    tokenize_char(), \n",
    "    stoi(char_vocab),\n",
    "    lambda x: torch.tensor(x)\n",
    ")\n",
    "\n",
    "trainset = TextClassificationDataset(\n",
    "    train_data,\n",
    "    char_vocab,\n",
    "    (lambda x: x, char_transform),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "regulation-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DataLoader\n",
    "def collate_fn(batch):\n",
    "    label, text = zip(*batch)\n",
    "    label = torch.LongTensor(label)\n",
    "    text = torch.stack(text)\n",
    "    #lens = list(map(lambda x: len(x[(x != 0).all(dim=1)]), text))\n",
    "    \n",
    "    return label, text\n",
    "\n",
    "pretrainloader = data.DataLoader(trainset, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-brazilian",
   "metadata": {},
   "source": [
    "### For training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "distant-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified a little to fit classification\n",
    "import itertools\n",
    "from torchtext.experimental.datasets import TextClassificationDataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.experimental.functional import sequential_transforms\n",
    "\n",
    "def build_char_vocab(data, index, bow=\"<w>\", eow=\"</w>\"):\n",
    "    \"\"\"\n",
    "    build character level vocabulary\n",
    "    \"\"\"\n",
    "    tok_list = [\n",
    "        [bow],\n",
    "        [eow],\n",
    "    ]\n",
    "    for line in data:\n",
    "        tokens = list(itertools.chain.from_iterable(line[index]))\n",
    "        tok_list.append(tokens)\n",
    "    return build_vocab_from_iterator(tok_list)\n",
    "\n",
    "\n",
    "def stoi(vocab):\n",
    "    \"\"\"\n",
    "    change string to index\n",
    "    \"\"\"\n",
    "    def func(tok_iter):\n",
    "        return [[vocab[char] for char in word]\\\n",
    "                for word in tok_iter]\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def tokenize_char(bow=\"<w>\", eow=\"</w>\", max_word_length=20):\n",
    "    \"\"\"\n",
    "    attach bow, eow token and pad with token\n",
    "    \"\"\"\n",
    "    def func(tok_iter):\n",
    "        result = np.empty((max(len_seq), max_word_length+2), dtype=object)\n",
    "        \n",
    "        # \"≥\" for padding\n",
    "        result[:len(tok_iter)] = [\n",
    "            [bow] + word + [eow] \\\n",
    "            + [\"<pad>\"] * (max_word_length - len(word)) \\\n",
    "            if len(word) < max_word_length \\\n",
    "            else [bow] + word[:max_word_length] + [eow]\n",
    "        for word in tok_iter]\n",
    "        \n",
    "        return result\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "diverse-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache training data for vocabulary construction\n",
    "train_data = [(line[0], [vocab.itos[ix] for ix in line[1]]) for line in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "optical-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store list of seq length for packing later\n",
    "len_seq = list(map(lambda x: len(x), train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "significant-baghdad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6760717lines [00:05, 1185785.41lines/s]\n"
     ]
    }
   ],
   "source": [
    "# Setup vocabularies (both words and chars)\n",
    "char_vocab = build_char_vocab(train_data, index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "based-firmware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the dataset with character level tokenization\n",
    "def char_tokenizer(words):\n",
    "    return [list(word) for word in words]\n",
    "\n",
    "char_transform = sequential_transforms(\n",
    "    char_tokenizer, \n",
    "    tokenize_char(), \n",
    "    stoi(char_vocab),\n",
    "    lambda x: torch.tensor(x)\n",
    ")\n",
    "\n",
    "trainset = TextClassificationDataset(\n",
    "    train_data,\n",
    "    char_vocab,\n",
    "    (lambda x: x, char_transform),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "adolescent-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DataLoader\n",
    "def collate_fn(batch):\n",
    "    label, text = zip(*batch)\n",
    "    label = torch.stack(label)\n",
    "    text = torch.stack(text)\n",
    "    #lens = list(map(lambda x: len(x[(x != 0).all(dim=1)]), text))\n",
    "    \n",
    "    return label, text\n",
    "\n",
    "trainloader = data.DataLoader(trainset, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-fight",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-chassis",
   "metadata": {},
   "source": [
    "### Bidirectional language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "laughing-mother",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2, 22])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = next(iter(pretrainloader))[1]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "median-notice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2, 22, 4])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = nn.Embedding(len(char_vocab), 4)(x)\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "blind-payroll",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4, 2, 22])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.permute(0,3,1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "other-duncan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 5, 2, 20])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3 = nn.Conv2d(4, 5, (1, 3))(x2.permute(0,3,1,2))\n",
    "x3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "swedish-binding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 5, 2, 1])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x4 = F.max_pool2d(x3, kernel_size=(1, x3.shape[3]))\n",
    "x4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "amber-marks",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 5, 2]), torch.Size([32, 10]), torch.Size([2, 32, 5]))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x5 = torch.squeeze(x4)\n",
    "x5.shape, x5.view(-1, ngrams*5).shape, x5.permute(2, 0, 1).shape  # (seq_len, batch, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "center-korean",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x6 = nn.Linear(ngrams*5, 10)(x5.view(-1, ngrams*5))\n",
    "x6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-chaos",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "worth-schema",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([1, 32, 6]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o, h = nn.LSTM(5, 6)(x5.permute(2, 0, 1))\n",
    "len(h), o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharConv(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(self, BiLM).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        CHAR_EMBEDDING_DIM = 16\n",
    "        self.char_embedding = nn.Embedding(len(char_vocab), CHAR_EMBEDDING_DIM)\n",
    "        \n",
    "        # Conv layers\n",
    "        self.convs = [\n",
    "            nn.Conv2d(CHAR_EMBEDDING_DIM, 4, 1),\n",
    "            nn.Conv2d(CHAR_EMBEDDING_DIM, 4, (1, 2)),\n",
    "            nn.Conv2d(CHAR_EMBEDDING_DIM, 8, (1, 3)),\n",
    "            nn.Conv2d(CHAR_EMBEDDING_DIM, 16, (1, 4)),\n",
    "            nn.Conv2d(CHAR_EMBEDDING_DIM, 32, (1, 5)),\n",
    "            nn.Conv2d(CHAR_EMBEDDING_DIM, 64, (1, 6)),\n",
    "            nn.Conv2d(CHAR_EMBEDDING_DIM, 128, (1, 7))\n",
    "        ]\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # character-level convolution\n",
    "        x = self.char_embedding(x).permute(0,3,1,2)\n",
    "        x = [conv(x) for conv in self.convs]\n",
    "        x = [F.max_pool2d(x_c, kernel_size=x_c.shape[3]) for x_c in x]\n",
    "        x = [torch.squeeze(x_p) for x_p in x]\n",
    "        x = torch.vstack(x)  # 1, n_batch, concat_length\n",
    "        x = x.permute(2, 0, 1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Bi-LSTM\n",
    "        self.lstm1 = nn.LSTM(256, 1024, bidirectional=True)\n",
    "        self.proj = nn.Linear(1024, 128, bias=False)\n",
    "        self.lstm2 = nn.LSTM(128, 1024, bidirectional=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-motivation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLM(nn.Module):\n",
    "    def __init__(self, char_cnn, bi_lstm):\n",
    "        \n",
    "        # Highway connection\n",
    "        CHAR_EMBEDDING_DIM = 16\n",
    "        self.highway = nn.Linear(ngrams * 256, 256)\n",
    "        self.transform = nn.Linear(ngrams * 256, 256)\n",
    "        self.char_cnn = char_cnn\n",
    "        self.bi_lstm = bi_lstm\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Character-level convolution\n",
    "        x = self.char_cnn(x)\n",
    "        x = x.view(1, -1)\n",
    "        \n",
    "        # highway\n",
    "        h = self.highway(x)\n",
    "        t_gate = torch.sigmoid(self.transform(x))\n",
    "        c_gate = 1 - t_gate\n",
    "        x = h * t_gate + x * c_gate\n",
    "        \n",
    "        # Bi-LSTM\n",
    "        self.bi_lstm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-complement",
   "metadata": {},
   "source": [
    "### ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-broad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "temporal-estate",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-disaster",
   "metadata": {},
   "source": [
    "### Pretrain bidirectional language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-natural",
   "metadata": {},
   "source": [
    "### Train ELMo for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-watch",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "3-2. ELMo",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
