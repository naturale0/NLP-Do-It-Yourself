{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "guilty-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import psi, polygamma, gammaln\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-possibility",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>3-1. Latent Dirichlet Allocation<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data:-Reuters\" data-toc-modified-id=\"Data:-Reuters-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data: Reuters</a></span></li><li><span><a href=\"#Model:-Basic-LDA\" data-toc-modified-id=\"Model:-Basic-LDA-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Model: Basic LDA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Variational-EM\" data-toc-modified-id=\"Variational-EM-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Variational EM</a></span><ul class=\"toc-item\"><li><span><a href=\"#E-step\" data-toc-modified-id=\"E-step-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>E-step</a></span></li><li><span><a href=\"#M-step\" data-toc-modified-id=\"M-step-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>M-step</a></span></li><li><span><a href=\"#Variational-lower-bound\" data-toc-modified-id=\"Variational-lower-bound-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Variational lower bound</a></span></li></ul></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Collapsed-Gibbs-Sampling\" data-toc-modified-id=\"Collapsed-Gibbs-Sampling-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Collapsed Gibbs Sampling</a></span></li></ul></li><li><span><a href=\"#Model:-Smoothed-LDA\" data-toc-modified-id=\"Model:-Smoothed-LDA-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Model: Smoothed LDA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Collaped-Gibbs-Sampling\" data-toc-modified-id=\"Collaped-Gibbs-Sampling-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Collaped Gibbs Sampling</a></span></li><li><span><a href=\"#Variational-EM\" data-toc-modified-id=\"Variational-EM-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Variational EM</a></span><ul class=\"toc-item\"><li><span><a href=\"#E-step\" data-toc-modified-id=\"E-step-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>E-step</a></span></li><li><span><a href=\"#M-step\" data-toc-modified-id=\"M-step-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>M-step</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-vertical",
   "metadata": {},
   "source": [
    "## Data: Reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-hungary",
   "metadata": {},
   "source": [
    "Reuters is a multi-class, multi-label dataset.\n",
    "\n",
    "* 90 classes\n",
    "* 10788 documents\n",
    "    * 7769 training documents\n",
    "    * 3019 testing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "green-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-liberty",
   "metadata": {},
   "source": [
    "* train-test split\n",
    ": The data is already splitted. Just sort it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ordered-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stopwords.words(\"english\")\n",
    "stops += [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\", \".\", \"!\", \"?\", \",\", \";\", \":\", \"[\", \"]\", \"{\", \"}\", \"-\", \"+\", \n",
    "    \"_\", \"/\", \"@\", \"#\", \"$\", \"%\", \"^\", \"&\", \"*\", \"(\", \")\", \"<\", \">\", \"|\", \"=\",\n",
    "    \".-\", \".,\", \"'\", '\"', ',\"'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "featured-tiger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "excellent-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = [], []\n",
    "vocab = []\n",
    "for file_id in reuters.fileids():\n",
    "    if file_id.startswith(\"train\"):\n",
    "        trainset.append([w.lower() for w in reuters.words(file_id) \\\n",
    "                         if (w.lower() not in stops) \\\n",
    "                         and (not w.isnumeric())])\n",
    "        vocab += trainset[-1]\n",
    "    else:\n",
    "        testset.append([w.lower() for w in reuters.words(file_id) \\\n",
    "                         if (w.lower() not in stops) \\\n",
    "                         and (not w.isnumeric())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "national-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(vocab))\n",
    "word_to_ix = {w: i for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "opened-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_ix(seq, vocab=vocab):\n",
    "    # len(vocab), which is the last index, is for the <unk> (unknown) token\n",
    "    unk_idx = len(vocab)\n",
    "    return np.array(list(map(lambda w: word_to_ix.get(w, unk_idx), seq)))\n",
    "\n",
    "data = {\n",
    "    \"train\": list(map(seq_to_ix, trainset)),\n",
    "    \"test\": list(map(seq_to_ix, testset))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "tender-polish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14079, 12862, 24273, 21913,  5652])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][0][:5]  # word indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-board",
   "metadata": {},
   "source": [
    "## Model: Basic LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-wilderness",
   "metadata": {},
   "source": [
    "### Variational EM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-gallery",
   "metadata": {},
   "source": [
    "For each document $\\mathbf{w}$ in a corpus $D$, generate\n",
    "\n",
    "$$\n",
    "N \\sim \\mathcal{P}(\\xi) \\\\\n",
    "\\theta \\sim \\text{Dir}(\\alpha)\n",
    "$$\n",
    "\n",
    "and for $n = 1, \\cdots, N$, generate\n",
    "\n",
    "$$\n",
    "z_n \\sim \\text{Multi}(\\theta) \\\\\n",
    "w_n \\sim P(w_n | z_n, \\beta)\n",
    "$$\n",
    "\n",
    "where $\\beta \\in \\mathbb{R}^{k \\times V}$, $\\beta_{ij} = P(w^j = 1| z^i = 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-vermont",
   "metadata": {},
   "source": [
    "* $\\alpha, \\beta$: hyperparameters (Dirichlet, Multinomial).\n",
    "* $N$: The number of words in the document. (ancillary variable)\n",
    "* $\\theta$: A topic mixture.\n",
    "* (For $i$ in $1\\cdots N$)\n",
    "  * $z_n$: A topic variable.\n",
    "  * $w_n$: A generated word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "adjusted-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lda(docs, n_topic, random_state=0):\n",
    "    global V, k, N, M, alpha, beta, gamma, phi\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    V = len(vocab)\n",
    "    k = n_topic  # number of topics\n",
    "    N = np.array([doc.shape[0] for doc in docs])\n",
    "    M = len(docs)\n",
    "\n",
    "    print(f\"V: {V}\\nk: {k}\\nN: {N[:10]}...\\nM: {M}\")\n",
    "\n",
    "    # initialize α, β\n",
    "    alpha = np.random.gamma(shape=100, scale=0.01, size=k) #np.random.rand(k)\n",
    "    beta = np.random.dirichlet(np.ones(V), k)\n",
    "\n",
    "    print(f\"α: dim {alpha.shape}\\nβ: dim {beta.shape}\")\n",
    "\n",
    "    # initialize ϕ, γ\n",
    "    ## ϕ: (M x max(N) x k) arrays with zero paddings on the right\n",
    "    gamma = alpha + np.ones((M, k)) * N.reshape(-1, 1) / k\n",
    "\n",
    "    phi = np.ones((M, max(N), k)) / k\n",
    "    for m, N_d in enumerate(N):\n",
    "        phi[m, N_d:, :] = 0  # zero padding for vectorized operations\n",
    "\n",
    "    print(f\"γ: dim {gamma.shape}\\nϕ: dim ({len(phi)}, N_d, {phi[0].shape[1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-phoenix",
   "metadata": {},
   "source": [
    "#### E-step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-algebra",
   "metadata": {},
   "source": [
    "Let $\\phi_d \\in \\mathbb{R}^{N \\times k}, \\gamma_d \\in \\mathbb{R}^k$ be variational parameters for $\\alpha, \\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-arcade",
   "metadata": {},
   "source": [
    "For a document $\\mathbf{w}_d$, $d = 1,\\cdots,M$,\n",
    "\n",
    "1. initialize $\\phi_{dni}^0 := 1/k$ for all $i,n$.\n",
    "2. initialize $\\gamma_{di}^0 := \\alpha_i + N/k$ for all $i$.\n",
    "3. **repeat until** convergence\n",
    "    1. for $n=1$ to $N$\n",
    "        1. for $i=1$ to $k$\n",
    "            1. $\\phi_{dni}^{t+1} := \\beta_{i\\mathbf{w}_{dn}}\\exp\\left(\\Psi(\\gamma_{di}^t) - \\Psi(\\sum_{j=1}^k \\gamma_{dj}^t)\\right)$\n",
    "        2. normalize $\\phi_{dn}^{t+1}$ to sum to 1\n",
    "    2. $\\gamma_d^{t+1} := \\alpha + \\sum_{n=1}^N \\phi_{dn}^{t+1}$\n",
    "    \n",
    "where $\\Psi$ is the first derivative of the $\\log\\Gamma$ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "reverse-velvet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def E_step(docs, phi, gamma, alpha, beta):\n",
    "    \"\"\"\n",
    "    Minorize the joint likelihood function via variational inference.\n",
    "    This is the E-step of variational EM algorithm for (smoothed) LDA.\n",
    "    \"\"\"\n",
    "    # optimize phi\n",
    "    for m in range(M):\n",
    "        phi[m, :N[m], :] = (beta[:, docs[m]] * np.exp(psi(gamma[m, :]) - psi(gamma[m, :].sum())).reshape(-1, 1)).T\n",
    "\n",
    "        # Normalize phi\n",
    "        phi[m, :N[m]] /= phi[m, :N[m]].sum(axis=1).reshape(-1, 1)\n",
    "        if np.any(np.isnan(phi)):\n",
    "            raise ValueError(\"phi nan\")\n",
    "\n",
    "    # optimize gamma\n",
    "    gamma = alpha + phi.sum(axis=1)\n",
    "\n",
    "    return phi, gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-referral",
   "metadata": {},
   "source": [
    "#### M-step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-strap",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta_{ij} \\propto \\sum_{d=1}^M \\sum_{n=1}^N \\phi_{dni} \\mathbf{w}_{dn}^j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-interest",
   "metadata": {},
   "source": [
    "$\\alpha$ is updated via Newton-Raphson method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-breeding",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\alpha_i} \n",
    "  = M\\left( \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right) - \\Psi(\\alpha_i) \\right)\n",
    "    - \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi\\left(\\sum_{j=1}^k \\gamma_{dj}\\right) \\right) \\\\\n",
    "\\frac{\\partial^2 L}{\\partial \\alpha_i \\alpha_j} = M \\left( \\Psi'\\left(\\sum_{j=1}^k \\alpha_j\\right) - \\delta(i,j) \\Psi'(\\alpha_i) \\right)\n",
    "$$\n",
    "\n",
    "where $\\delta(i,j) = 1$ if $i=j$, $0$ otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "reflected-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step(docs, phi, gamma, alpha, beta, M):\n",
    "    \"\"\"\n",
    "    maximize the lower bound of the likelihood.\n",
    "    This is the M-step of variational EM algorithm for (smoothed) LDA.\n",
    "    \n",
    "    update of alpha follows from appendix A.2 of Blei et al., 2003.\n",
    "    \"\"\"\n",
    "    # update alpha\n",
    "    alpha = _update(alpha, gamma, M)\n",
    "    \n",
    "    # update beta\n",
    "    for j in range(V):\n",
    "        beta[:, j] = np.array([_inner_sum(docs, phi, m, j) for m in range(M)]).sum(axis=0)\n",
    "    beta /= beta.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "south-lease",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def _update(var, vi_var, const, max_iter=10000, tol=1e-4):\n",
    "    \"\"\"\n",
    "    From appendix A.2 of Blei et al., 2003.\n",
    "    For hessian with shape `H = diag(h) + 1z1'`\n",
    "    \n",
    "    To update alpha, input var=alpha and vi_var=gamma, const=M.\n",
    "    To update eta, input var=eta and vi_var=lambda, const=k.\n",
    "    \"\"\"\n",
    "    for _ in range(max_iter):\n",
    "        # store old value\n",
    "        var0 = var.copy()\n",
    "        \n",
    "        # g: gradient \n",
    "        psi_sum = psi(vi_var.sum(axis=1)).reshape(-1, 1)\n",
    "        g = const * (psi(var.sum()) - psi(var)) \\\n",
    "            + (psi(vi_var) - psi_sum).sum(axis=0)\n",
    "\n",
    "        # H = diag(h) + 1z1'\n",
    "        z = const * polygamma(1, var.sum())  # z: Hessian constant component\n",
    "        h = -const * polygamma(1, var)       # h: Hessian diagonal component\n",
    "        c = (g / h).sum() / (1./z + (1./h).sum())\n",
    "\n",
    "        # update var\n",
    "        var -= (g - c) / h\n",
    "        \n",
    "        # check convergence\n",
    "        err = np.mean((var - var0) ** 2)\n",
    "        crit = err < tol ** 2\n",
    "        if crit:\n",
    "            break\n",
    "    else:\n",
    "        warnings.warn(f\"max_iter={max_iter} reached: values might not be optimal.\")\n",
    "    \n",
    "    #print(err)\n",
    "    return var\n",
    "\n",
    "def _inner_sum(docs, phi, m, j):\n",
    "    # doc = np.zeros(docs[m].shape[0] * V, dtype=int)\n",
    "    # doc[np.arange(0, docs[m].shape[0] * V, V) + docs[m]] = 1\n",
    "    # doc = doc.reshape(-1, V)\n",
    "    # lam += phi[m, :N[m], :].T @ doc\n",
    "    return (docs[m] == j) @ phi[m, :N[m], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-hollywood",
   "metadata": {},
   "source": [
    "#### Variational lower bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-veteran",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "L(\\gamma, \\phi; \\alpha, \\beta)\n",
    "  &= \\log\\Gamma(\\sum_{j=1}^k \\alpha_j) - \\sum_{i=1}^k \\log\\Gamma(\\alpha_i) + \\sum_{i=1}^k (\\alpha_i - 1) \\left(\\Psi(\\gamma_i) - \\Psi(\\sum_{i=1}^k \\gamma_i)\\right) \\\\\n",
    "  &+ \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\left(\\Psi(\\gamma_i) - \\Psi(\\sum_{i=1}^k \\gamma_i)\\right) \\\\\n",
    "  &+ \\sum_{n=1}^N \\sum_{i=1}^k \\sum_{j=1}^V \\phi_{ni} \\mathbf{w}_{n}^j \\log\\beta_{ij} \\\\\n",
    "  &- \\log\\Gamma(\\sum_{i=1}^k \\gamma_i) + \\sum_{i=1}^k \\log\\Gamma(\\gamma_i) - \\sum_{i=1}^k (\\gamma_i - 1) \\left(\\Psi(\\gamma_i) - \\Psi(\\sum_{i=1}^k \\gamma_i)\\right) \\\\\n",
    "  &- \\sum_{n=1}^N \\sum_{i=1}^k \\phi_{ni} \\log\\phi_{ni}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "colored-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dg(gamma, d, i):\n",
    "    \"\"\"\n",
    "    E[log θ_t] where θ_t ~ Dir(gamma)\n",
    "    \"\"\"\n",
    "    return psi(gamma[d, i]) - psi(np.sum(gamma[d, :]))\n",
    "\n",
    "\n",
    "def dl(lam, i, w_n):\n",
    "    \"\"\"\n",
    "    E[log β_t] where β_t ~ Dir(lam)\n",
    "    \"\"\"\n",
    "    return psi(lam[i, w_n]) - psi(np.sum(lam[i, :]))\n",
    "\n",
    "def vlb(docs, phi, gamma, alpha, beta, M, N, k):\n",
    "    \"\"\"\n",
    "    Average variational lower bound for joint log likelihood.\n",
    "    \"\"\"\n",
    "    lb = 0\n",
    "    for d in range(M):\n",
    "        lb += (\n",
    "            gammaln(np.sum(alpha))\n",
    "            - np.sum(gammaln(alpha))\n",
    "            + np.sum([(alpha[i] - 1) * dg(gamma, d, i) for i in range(k)])\n",
    "        )\n",
    "\n",
    "        lb += (\n",
    "            gammaln(np.sum(gamma[d, :]))\n",
    "            - np.sum(gammaln(gamma[d, :]))\n",
    "            + np.sum([(gamma[d, i] - 1) * dg(gamma, d, i) for i in range(k)])\n",
    "        )\n",
    "\n",
    "        for n in range(N[d]):\n",
    "            w_n = int(docs[d][n])\n",
    "\n",
    "            lb += np.sum([phi[d][n, i] * dg(gamma, d, i) for i in range(k)])\n",
    "            lb += np.sum([phi[d][n, i] * np.log(beta[i, w_n]) for i in range(k)])\n",
    "            lb += np.sum([phi[d][n, i] * np.log(phi[d][n, i]) for i in range(k)])\n",
    "\n",
    "    return lb / M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-houston",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-confirmation",
   "metadata": {},
   "source": [
    "Only on 100 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "seventh-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data[\"train\"][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "latin-switch",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V: 24714\n",
      "k: 5\n",
      "N: [264 127  59  74  55  18 117  43  86  76]...\n",
      "M: 100\n",
      "α: dim (5,)\n",
      "β: dim (5, 24714)\n",
      "γ: dim (100, 5)\n",
      "ϕ: dim (100, N_d, 5)\n"
     ]
    }
   ],
   "source": [
    "init_lda(docs, n_topic=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "female-mouse",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 000:  variational_lb: -582.986\n",
      " 001:  variational_lb: -577.172\n",
      " 002:  variational_lb: -571.734\n",
      " 003:  variational_lb: -564.618\n",
      " 004:  variational_lb: -553.614\n",
      " 005:  variational_lb: -536.302\n",
      " 006:  variational_lb: -511.802\n",
      " 007:  variational_lb: -482.697\n",
      " 008:  variational_lb: -453.737\n",
      " 009:  variational_lb: -428.951\n",
      " 010:  variational_lb: -409.294\n",
      " 011:  variational_lb: -393.378\n",
      " 012:  variational_lb: -378.997\n",
      " 013:  variational_lb: -364.497\n",
      " 014:  variational_lb: -350.007\n",
      " 015:  variational_lb: -335.778\n",
      " 016:  variational_lb: -322.113\n",
      " 017:  variational_lb: -308.693\n",
      " 018:  variational_lb: -295.426\n",
      " 019:  variational_lb: -282.391\n",
      " 020:  variational_lb: -269.388\n",
      " 021:  variational_lb: -255.742\n",
      " 022:  variational_lb: -240.965\n",
      " 023:  variational_lb: -225.460\n",
      " 024:  variational_lb: -209.621\n",
      " 025:  variational_lb: -194.219\n",
      " 026:  variational_lb: -179.056\n",
      " 027:  variational_lb: -163.616\n",
      " 028:  variational_lb: -148.061\n",
      " 029:  variational_lb: -132.800\n",
      " 030:  variational_lb: -117.660\n",
      " 031:  variational_lb: -101.955\n",
      " 032:  variational_lb: -86.497\n",
      " 033:  variational_lb: -71.473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-1f6ac6fbd577>:36: RuntimeWarning: divide by zero encountered in log\n",
      "  lb += np.sum([phi[d][n, i] * np.log(beta[i, w_n]) for i in range(k)])\n",
      "<ipython-input-79-1f6ac6fbd577>:36: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  lb += np.sum([phi[d][n, i] * np.log(beta[i, w_n]) for i in range(k)])\n",
      "<ipython-input-79-1f6ac6fbd577>:37: RuntimeWarning: divide by zero encountered in log\n",
      "  lb += np.sum([phi[d][n, i] * np.log(phi[d][n, i]) for i in range(k)])\n",
      "<ipython-input-79-1f6ac6fbd577>:37: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  lb += np.sum([phi[d][n, i] * np.log(phi[d][n, i]) for i in range(k)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN detected: lb\n",
      " ========== TRAINING FINISHED ==========\n",
      "CPU times: user 2min 49s, sys: 493 ms, total: 2min 50s\n",
      "Wall time: 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "N_EPOCH = 1000\n",
    "TOL = .1\n",
    "\n",
    "verbose = True\n",
    "lb = -np.inf\n",
    "\n",
    "for epoch in range(N_EPOCH): \n",
    "    # store old value\n",
    "    lb_old = lb\n",
    "    \n",
    "    # Variational EM\n",
    "    phi, gamma = E_step(docs, phi, gamma, alpha, beta)\n",
    "    alpha, beta = M_step(docs, phi, gamma, alpha, beta, M)\n",
    "    \n",
    "    # check anomaly\n",
    "    if np.any(np.isnan(alpha)):\n",
    "        print(\"NaN detected: alpha\")\n",
    "        break\n",
    "    \n",
    "    # check convergence\n",
    "    lb = vlb(docs, phi, gamma, alpha, beta, M, N, k)\n",
    "    err = abs(lb - lb_old)\n",
    "    \n",
    "    # check anomaly\n",
    "    if np.isnan(lb):\n",
    "        print(\"NaN detected: lb\")\n",
    "        break\n",
    "        \n",
    "    if verbose:\n",
    "        print(f\"{epoch: 04}:  variational_lb: {lb: .3f},  error: {err: .3f}\")\n",
    "    \n",
    "    if err < TOL:\n",
    "        break\n",
    "else:\n",
    "    warnings.warn(f\"max_iter reached: values might not be optimal.\")\n",
    "\n",
    "print(\" ========== TRAINING FINISHED ==========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-bread",
   "metadata": {},
   "source": [
    "Training result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-burke",
   "metadata": {},
   "source": [
    "1. Topic extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "inner-punch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_most_important(beta_i, n=30):\n",
    "    \"\"\"\n",
    "    find the index of the largest `n` values in a list\n",
    "    \"\"\"\n",
    "    \n",
    "    max_values = beta_i.argsort()[-n:][::-1]\n",
    "    return np.array(vocab)[max_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "foreign-edwards",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC 00: ['said' 'oil' 'dlrs' 'bp' 'u' 'lt' 'standard' ',\"' 'offer' 'year']\n",
      "TOPIC 01: ['vs' 'mln' 'loss' 'cts' 'net' 'shr' 'year' 'profit' 'revs' 'dlrs']\n",
      "TOPIC 02: ['said' 'lt' 'company' 'cts' 'quarter' 'stock' 'share' 'corp' 'dlrs'\n",
      " 'april']\n",
      "TOPIC 03: ['said' 'dlrs' 'new' 'cocoa' 'stock' 'crop' 'bales' 'farmers' 'u' 'buffer']\n",
      "TOPIC 04: ['said' 'pct' 'billion' 'bank' 'heller' 'mln' 'year' 'financial' 'cubic'\n",
      " 'company']\n"
     ]
    }
   ],
   "source": [
    "for i in range(k):\n",
    "    print(f\"TOPIC {i:02}: {n_most_important(beta[i], 10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-agenda",
   "metadata": {},
   "source": [
    "2. Topic-word distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "falling-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "?sns.clustermap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "chinese-distinction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(121)\n",
    "sns.heatmap(beta.T[:300], xticklabels=[], yticklabels=[])\n",
    "plt.xlabel(\"Topics\", fontsize=14)\n",
    "plt.ylabel(\"Words[:300]\", fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.heatmap(gamma, xticklabels=[], yticklabels=[])\n",
    "plt.xlabel(\"Topics\", fontsize=14)\n",
    "plt.ylabel(\"Documents\", fontsize=14)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-secret",
   "metadata": {},
   "source": [
    "### Collapsed Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-macro",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-fishing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-delay",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-fellow",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-lender",
   "metadata": {},
   "source": [
    "## Model: Smoothed LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-cause",
   "metadata": {},
   "source": [
    "For each document $\\mathbf{w}$ in a corpus $D$, generate\n",
    "\n",
    "$$\n",
    "N \\sim \\mathcal{P}(\\xi) \\\\\n",
    "\\beta \\sim \\text{Dir}(\\lambda) \\\\\n",
    "\\theta \\sim \\text{Dir}(\\alpha)\n",
    "$$\n",
    "\n",
    "and for $n = 1, \\cdots, N$, generate\n",
    "\n",
    "$$\n",
    "z_n \\sim \\text{Multi}(\\theta) \\\\\n",
    "w_n \\sim P(w_n | z_n, \\beta)\n",
    "$$\n",
    "\n",
    "where $\\beta \\in \\mathbb{R}^{k \\times V}$, $\\beta_{ij} = P(w^j = 1| z^i = 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-porter",
   "metadata": {},
   "source": [
    "* $\\alpha, \\eta$: Dirichlet hyperparameters.\n",
    "* $\\beta$: Unsmoothed multinomial hyperparameter.\n",
    "* $N$: The number of words in the document. (ancillary variable)\n",
    "* $\\theta$: A topic mixture.\n",
    "* (For $i$ in $1\\cdots N$)\n",
    "  * $z_n$: A topic variable.\n",
    "  * $w_n$: A generated word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-facility",
   "metadata": {},
   "source": [
    "### Collaped Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-telling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hindu-telescope",
   "metadata": {},
   "source": [
    "### Variational EM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-favor",
   "metadata": {},
   "source": [
    "#### E-step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-thing",
   "metadata": {},
   "source": [
    "Let $\\phi_d \\in \\mathbb{R}^{N \\times k}, \\gamma_d \\in \\mathbb{R}^k, \\lambda \\in \\mathbb{R}^{k \\times V}$ be variational parameters for $\\alpha, \\beta, \\eta$.  \n",
    "Suppose further that for $\\beta \\in \\mathbb{R}^{k \\times V}$, $\\beta_i^0 \\sim \\text{Dir}(\\lambda^0)$ where $\\lambda_i^0 = \\eta$ for all $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-digit",
   "metadata": {},
   "source": [
    "For a document $\\mathbf{w}_d$, $d = 1,\\cdots,M$,\n",
    "\n",
    "1. initialize $\\phi_{dni}^0 := 1/k$ for all $i,n$.\n",
    "2. initialize $\\gamma_{di} := \\alpha_i + N/k$ for all $i$.\n",
    "3. **repeat until** convergence\n",
    "    1. for $n=1$ to $N$\n",
    "        1. for $i=1$ to $k$\n",
    "            1. $\\phi_{dni}^{t+1} := \\exp\\left(\\Psi(\\lambda_{iw_{dn}}^t) - \\Psi(\\sum_{j=1}^V \\lambda_{ij}^t) + \\Psi(\\gamma_{di}^t) - \\Psi(\\sum_{j=1}^k \\gamma_{dj}^t)\\right)$\n",
    "            1. for $j=1$ to $V$\n",
    "                1. $\\lambda_{ij} = \\eta + \\sum_{d=1}^M \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j$\n",
    "        2. normalize $\\phi_{dn}^{t+1}$ to sum to 1\n",
    "    2. $\\gamma_d^{t+1} := \\alpha + \\sum_{n=1}^N \\phi_{dn}^{t+1}$\n",
    "    \n",
    "where $\\Psi$ is the first derivative of the $\\log\\Gamma$ function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-occasion",
   "metadata": {},
   "source": [
    "#### M-step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-swing",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta_{ij} \\propto \\sum_{d=1}^M \\sum_{n=1}^N \\phi_{dni} \\mathbf{w}_{dn}^j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-flood",
   "metadata": {},
   "source": [
    "$\\alpha$ is updated via Newton-Raphson method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-friday",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\alpha_i} \n",
    "  = M\\left( \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right) - \\Psi(\\alpha_i) \\right)\n",
    "    - \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi\\left(\\sum_{j=1}^k \\gamma_{dj}\\right) \\right) \\\\\n",
    "\\frac{\\partial^2 L}{\\partial \\alpha_i \\alpha_j} = M \\left( \\Psi'\\left(\\sum_{j=1}^k \\alpha_j\\right) - \\delta(i,j) \\Psi'(\\alpha_i) \\right)\n",
    "$$\n",
    "\n",
    "where $\\delta(i,j) = 1$ if $i=j$, $0$ otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "later-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothedLDA:\n",
    "    \n",
    "    def __init__(self, docs, vocab, k):\n",
    "        self.docs = docs\n",
    "        \n",
    "        self.V = len(vocab)\n",
    "        self.k = k  # number of topics\n",
    "        self.N = np.array([doc.shape[0] for doc in docs])\n",
    "        self.M = len(docs)\n",
    "        \n",
    "        V = self.V\n",
    "        N = self.N\n",
    "        M = self.M\n",
    "        \n",
    "        # initialize model parameters\n",
    "        ##self.beta = np.ones((k, V)) / V\n",
    "        self.alpha = np.random.gamma(100, 0.01, k)\n",
    "        self.eta = np.ones(V)\n",
    "\n",
    "        # initialize variational parameters\n",
    "        # ϕ: (M x max(N) x k) arrays with zero paddings on the right\n",
    "        self.phi = [np.ones((N[d], k)) / k for d in range(M)]\n",
    "        self.gamma = alpha + (N / k).reshape(-1, 1)\n",
    "        self.lam = np.random.gamma(shape=100, scale=0.01, size=(k, V))\n",
    "        \n",
    "        \n",
    "    def _update_phi(self):\n",
    "        \"\"\"\n",
    "        Update variational parameter phi\n",
    "        ϕ_{n, j} ∝ e^[ (Ψ(λ_j) - Ψ(Σλ_j)) + ( Ψ(γ_j) - Ψ(Σγ_j) ) ]\n",
    "        \"\"\"\n",
    "        M = self.M\n",
    "        N = self.N\n",
    "        k = self.k\n",
    "\n",
    "        phi = self.phi\n",
    "        gamma = self.gamma\n",
    "        docs = self.docs\n",
    "\n",
    "        for d in range(M):\n",
    "            for n in range(N[d]):\n",
    "                for i in range(k):\n",
    "                    w_n = int(docs[d][n])\n",
    "                    phi[d][n, i] = np.exp(dl(lam, i, w_n) + dg(gamma, d, i))\n",
    "\n",
    "                # Normalize over topics\n",
    "                phi[d][n, :] = phi[d][n, :] / np.sum(phi[d][n, :])\n",
    "                \n",
    "        return phi\n",
    "    \n",
    "    def _update_gamma(self):\n",
    "        \"\"\"\n",
    "        Update variational parameter gamma\n",
    "        γ_t = α_t + Σ_{n=1}^{N_d} ϕ_{t, n}\n",
    "        \"\"\"\n",
    "        M = self.M\n",
    "        phi = self.phi\n",
    "        alpha = self.alpha\n",
    "\n",
    "        gamma = alpha + np.array(\n",
    "            list(map(lambda x: x.sum(axis=0), phi))\n",
    "        )\n",
    "        \n",
    "        return gamma\n",
    "    \n",
    "    \n",
    "    def _update_lam(self):\n",
    "        V = self.V\n",
    "        N = self.N\n",
    "        M = self.M\n",
    "        phi = self.phi\n",
    "        lam = self.lam\n",
    "        eta = self.eta\n",
    "        docs = self.docs\n",
    "        \n",
    "        lam[:] = eta\n",
    "        for d in range(M):  #, desc=\"MINORIZE lam\"):\n",
    "            doc = np.zeros(N[d] * V, dtype=int)\n",
    "            doc[np.arange(0, N[d] * V, V) + docs[d]] = 1\n",
    "            doc = doc.reshape(-1, V)\n",
    "            \n",
    "            lam += phi[d].T @ doc\n",
    "        \n",
    "        return lam\n",
    "        \n",
    "    \n",
    "    def _update_alpha(self, max_iter=1000, tol=0.1):\n",
    "        \"\"\"\n",
    "        Update alpha with linear time Newton-Raphson.\n",
    "        \"\"\"\n",
    "        M = self.M\n",
    "        k = self.k\n",
    "\n",
    "        alpha = self.alpha\n",
    "        gamma = self.gamma\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            alpha_old = alpha\n",
    "\n",
    "            #  Calculate gradient\n",
    "            g = M * (psi(np.sum(alpha)) - psi(alpha)) +\\\n",
    "                (psi(gamma) - psi(np.sum(gamma, axis=1)).reshape(-1, 1)).sum(axis=0)\n",
    "\n",
    "            #  Calculate Hessian diagonal component\n",
    "            h = -M * polygamma(1, alpha)\n",
    "\n",
    "            #  Calculate Hessian constant component\n",
    "            z = M * polygamma(1, np.sum(alpha))\n",
    "\n",
    "            #  Calculate constant\n",
    "            c = np.sum(g / h) / (z ** (-1.0) + np.sum(h ** (-1.0)))\n",
    "\n",
    "            #  Update alpha\n",
    "            alpha = alpha - (g - c) / h\n",
    "            \n",
    "            #  Check convergence\n",
    "            if np.sqrt(np.mean(np.square(alpha - alpha_old))) < tol:\n",
    "                break\n",
    "        else:\n",
    "            warnings.warn(\"_update_alpha(): max_iter reached.\")\n",
    "\n",
    "        return alpha\n",
    "    \n",
    "    \n",
    "    def _update_eta(self, max_iter=1000, tol=0.1):\n",
    "        \"\"\"\n",
    "        Update eta with linear time Newton-Raphson.\n",
    "        \"\"\"\n",
    "        M = self.M\n",
    "        k = self.k\n",
    "\n",
    "        eta = self.eta\n",
    "        lam = self.lam\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            eta_old = eta\n",
    "\n",
    "            #  Calculate gradient\n",
    "            g = k * (psi(np.sum(eta)) - psi(eta)) +\\\n",
    "                (psi(lam) - psi(np.sum(lam, axis=1)).reshape(-1, 1)).sum(axis=0)\n",
    "\n",
    "            #  Calculate Hessian diagonal component\n",
    "            h = -k * polygamma(1, eta)\n",
    "\n",
    "            #  Calculate Hessian constant component\n",
    "            z = k * polygamma(1, np.sum(eta))\n",
    "\n",
    "            #  Calculate constant\n",
    "            c = np.sum(g / h) / (z ** (-1.0) + np.sum(h ** (-1.0)))\n",
    "\n",
    "            #  Update alpha\n",
    "            eta = eta - (g - c) / h\n",
    "\n",
    "            #  Check convergence\n",
    "            if np.sqrt(np.mean(np.square(eta - eta_old))) < tol:\n",
    "                break\n",
    "        else:\n",
    "            warnings.warn(\"_update_eta(): max_iter reached.\")\n",
    "\n",
    "        return eta\n",
    "    \n",
    "    \n",
    "    def _E_step(self):\n",
    "        \"\"\"\n",
    "        E-step of the variational EM algorithm.\n",
    "        Update ϕ, γ, λ.\n",
    "        \"\"\"\n",
    "        self.phi = self._update_phi()\n",
    "        self.gamma = self._update_gamma()\n",
    "        self.lam = self._update_lam()\n",
    "        \n",
    "        \n",
    "    def _M_step(self):\n",
    "        \"\"\"\n",
    "        M-step of the variational EM algorithm.\n",
    "        Update α, η.\n",
    "        \"\"\"\n",
    "        self.alpha = self._update_alpha()\n",
    "        self.eta = self._update_eta()\n",
    "    \n",
    "    \n",
    "    def vlb(self):\n",
    "        \"\"\"\n",
    "        lower bound from variational inference\n",
    "        \"\"\"\n",
    "        phi = self.phi\n",
    "        gamma = self.gamma\n",
    "        lam = self.lam\n",
    "        alpha = self.alpha\n",
    "        eta = self.eta\n",
    "        docs = self.docs\n",
    "        \n",
    "        M = self.M\n",
    "        k = self.k\n",
    "        N = self.N\n",
    "        \n",
    "        a0, a1, a2, a3_1, a3_2, a4, a5 = 0., 0., 0., 0., 0., 0., 0.\n",
    "        for d in range(M):\n",
    "            a0 += (\n",
    "                k * (\n",
    "                    gammaln(np.sum(eta)) \n",
    "                    - np.sum(gammaln(eta))\n",
    "                )\n",
    "                + np.sum([(eta[j] - 1) * dl(lam, i, j) for j in range(V) for i in range(k)])\n",
    "            )\n",
    "            a1 += (\n",
    "                gammaln(np.sum(alpha))\n",
    "                - np.sum(gammaln(alpha))\n",
    "                + np.sum([(alpha[i] - 1) * dg(gamma, d, i) for i in range(k)])\n",
    "            )\n",
    "\n",
    "            a4 += (\n",
    "                gammaln(np.sum(gamma[d, :]))\n",
    "                - np.sum(gammaln(gamma[d, :]))\n",
    "                + np.sum([(gamma[d, i] - 1) * dg(gamma, d, i) for i in range(k)])\n",
    "            )\n",
    "            \n",
    "            for i in range(k):\n",
    "                for j in range(V):\n",
    "                    a3_2 += (\n",
    "                        gammaln(np.sum(lam[i, j]))\n",
    "                        - np.sum(gammaln(lam[i, :]))\n",
    "                        + np.sum((lam[i, j] - 1) * dl(lam, i, j))\n",
    "                    )\n",
    "\n",
    "            for n in range(N[d]):\n",
    "                w_n = int(docs[d][n])\n",
    "                a2 += np.sum([phi[d][n, i] * dg(gamma, d, i) for i in range(k)])\n",
    "                a3_1 += np.sum([phi[d][n, i] * dl(lam, i, w_n) for i in range(k)])\n",
    "                a5 += np.sum([phi[d][n, i] * np.log(phi[d][n, i]) for i in range(k)])\n",
    "\n",
    "        return a0 + a1 + a2 + a3_1 - a3_2 - a4 - a5\n",
    "    \n",
    "    \n",
    "    def train(self, max_iter=1000, tol=5, verbose=True):\n",
    "        vlb = -np.inf\n",
    "        \n",
    "        for it in range(max_iter):\n",
    "            old_vlb = vlb\n",
    "            self._E_step()\n",
    "            self._M_step()\n",
    "            \n",
    "            vlb = self.vlb()\n",
    "            err = vlb - old_vlb\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Iteration {it+1}: {vlb: .3f} (delta: {err: .2f})\") \n",
    "            \n",
    "            if err < tol:\n",
    "                break\n",
    "        else:\n",
    "            warnings.warn(\"max_iter reached.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-miami",
   "metadata": {},
   "source": [
    "```python\n",
    "lda = SmoothedLDA(docs, vocab, k)\n",
    "lda.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-offer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "3-1. Latent Dirichlet Allocation",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
