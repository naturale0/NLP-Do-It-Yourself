{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import digamma, polygamma\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-possibility",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>3-1. Latent Dirichlet Allocation<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data:-Reuters\" data-toc-modified-id=\"Data:-Reuters-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data: Reuters</a></span></li><li><span><a href=\"#Model:-Smoothed-LDA\" data-toc-modified-id=\"Model:-Smoothed-LDA-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Model: Smoothed LDA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Variational-EM\" data-toc-modified-id=\"Variational-EM-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Variational EM</a></span><ul class=\"toc-item\"><li><span><a href=\"#E-step\" data-toc-modified-id=\"E-step-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>E-step</a></span></li><li><span><a href=\"#M-step\" data-toc-modified-id=\"M-step-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>M-step</a></span></li></ul></li></ul></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Training</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-vertical",
   "metadata": {},
   "source": [
    "## Data: Reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-hungary",
   "metadata": {},
   "source": [
    "Reuters is a multi-class, multi-label dataset.\n",
    "\n",
    "* 90 classes\n",
    "* 10788 documents\n",
    "    * 7769 training documents\n",
    "    * 3019 testing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-liberty",
   "metadata": {},
   "source": [
    "* train-test split\n",
    ": The data is already splitted. Just sort it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = [], []\n",
    "vocab = []\n",
    "for file_id in reuters.fileids():\n",
    "    if file_id.startswith(\"train\"):\n",
    "        trainset.append([w for w in reuters.words(file_id) if w not in stops])\n",
    "        vocab += trainset[-1]\n",
    "    else:\n",
    "        testset.append([w for w in reuters.words(file_id) if w not in stops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "knowing-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(vocab)\n",
    "word_to_ix = {k: v for v, k in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "opened-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_ix(seq, vocab=vocab):\n",
    "    # len(vocab), which is the last index, is for the <unk> (unknown) token\n",
    "    unk_idx = len(vocab)\n",
    "    return np.array(list(map(lambda w: word_to_ix.get(w, unk_idx), seq)))\n",
    "\n",
    "data = {\n",
    "    \"train\": list(map(seq_to_ix, trainset)),\n",
    "    \"test\": list(map(seq_to_ix, testset))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tender-polish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14373,  9509, 14991, 24125,  2348])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-evening",
   "metadata": {},
   "source": [
    "## Model: Smoothed LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-viking",
   "metadata": {},
   "source": [
    "For each document $\\mathbf{w}$ in a corpus $D$, generate\n",
    "\n",
    "$$\n",
    "N \\sim \\mathcal{P}(\\xi) \\\\\n",
    "\\beta \\sim \\text{Dir}(\\lambda) \\\\\n",
    "\\theta \\sim \\text{Dir}(\\alpha)\n",
    "$$\n",
    "\n",
    "and for $n = 1, \\cdots, N$, generate\n",
    "\n",
    "$$\n",
    "z_n \\sim \\text{Multi}(\\theta) \\\\\n",
    "w_n \\sim P(w_n | z_n, \\beta)\n",
    "$$\n",
    "\n",
    "where $\\beta \\in \\mathbb{R}^{k \\times V}$, $\\beta_{ij} = P(w^j = 1| z^i = 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-hands",
   "metadata": {},
   "source": [
    "* $\\alpha, \\eta$: Dirichlet hyperparameters.\n",
    "* $\\beta$: Unsmoothed multinomial hyperparameter.\n",
    "* $N$: The number of words in the document. (ancillary variable)\n",
    "* $\\theta$: A topic mixture.\n",
    "* (For $i$ in $1\\cdots N$)\n",
    "  * $z_n$: A topic variable.\n",
    "  * $w_n$: A generated word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-telescope",
   "metadata": {},
   "source": [
    "### Variational EM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-favor",
   "metadata": {},
   "source": [
    "#### E-step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-thing",
   "metadata": {},
   "source": [
    "Let $\\phi_d \\in \\mathbb{R}^{N \\times k}, \\gamma_d \\in \\mathbb{R}^k, \\lambda \\in \\mathbb{R}^{k \\times V}$ be variational parameters for $\\alpha, \\beta, \\eta$.  \n",
    "Suppose further that for $\\beta \\in \\mathbb{R}^{k \\times V}$, $\\beta_i^0 \\sim \\text{Dir}(\\lambda^0)$ where $\\lambda_i^0 = \\eta$ for all $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-digit",
   "metadata": {},
   "source": [
    "For a document $\\mathbf{w}_d$, $d = 1,\\cdots,M$,\n",
    "\n",
    "1. initialize $\\phi_{dni}^0 := 1/k$ for all $i,n$.\n",
    "2. initialize $\\gamma_{di} := \\alpha_i + N/k$ for all $i$.\n",
    "3. **repeat until** convergence\n",
    "    1. for $n=1$ to $N$\n",
    "        1. for $i=1$ to $k$\n",
    "            1. $\\phi_{dni}^{t+1} := \\beta_{iw_n} \\exp\\left(\\Psi(\\gamma_{di}^t) - \\Psi(\\sum_{j=1}^k \\gamma_{dj}^t)\\right)$\n",
    "            1. for $j=1$ to $V$\n",
    "                1. $\\lambda_{ij} = \\eta + \\sum_{d=1}^M \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j$\n",
    "        2. normalize $\\phi_{dn}^{t+1}$ to sum to 1\n",
    "    2. $\\gamma_d^{t+1} := \\alpha + \\sum_{n=1}^N \\phi_{dn}^{t+1}$\n",
    "    \n",
    "where $\\Psi$ is the first derivative of the $\\log\\Gamma$ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "suspended-albany",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V: 35105\n",
      "k: 90\n",
      "N: [472 189  96 ...  23  91  50]\n",
      "M: 7769\n"
     ]
    }
   ],
   "source": [
    "V = len(vocab)\n",
    "k = 90  # number of topics\n",
    "N = np.array([len(doc) for doc in data[\"train\"]])\n",
    "M = len(data[\"train\"])\n",
    "\n",
    "print(f\"V: {V}\\nk: {k}\\nN: {N}\\nM: {M}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "imported-yemen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "α: dim (90,)\n",
      "β: dim (90, 35105)\n",
      "η: dim (90, 35105)\n"
     ]
    }
   ],
   "source": [
    "# initialize α, β\n",
    "α = np.ones(k)\n",
    "β = np.ones((k, V)) / V\n",
    "η = np.ones((k, V)) / V\n",
    "\n",
    "print(f\"α: dim {α.shape}\\nβ: dim {β.shape}\\nη: dim {η.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "numerous-brain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "γ: dim (7769, 90)\n",
      "ϕ: dim (7769, N_d, 90)\n",
      "λ: dim (90, 35105)\n"
     ]
    }
   ],
   "source": [
    "# initialize ϕ, γ\n",
    "## ϕ: (M x max(N) x k) arrays with zero paddings on the right\n",
    "γ = α + np.ones((M, k)) * N.reshape(-1, 1) / k\n",
    "\n",
    "ϕ = np.ones((M, max(N), k))\n",
    "for m, N_d in enumerate(N):\n",
    "    ϕ[m, N_d:, :] = 0  # zero padding for vectorized operations\n",
    "\n",
    "λ = η + np.ones((k, V)) * M / V\n",
    "\n",
    "print(f\"γ: dim {γ.shape}\\nϕ: dim ({len(ϕ)}, N_d, {ϕ[0].shape[1]})\\nλ: dim {λ.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "middle-blackjack",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-franchise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-quilt",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-locking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-construction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 914/7769 [01:13<33:03,  3.46it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def inner_sum2(m):\n",
    "    return (np.array([words[m] == j for j in range(V)]) @ ϕ[m, :N[m], :]).T\n",
    "\n",
    "with Pool(16) as p:\n",
    "    res = list(tqdm(p.imap(inner_sum2, range(M)), total=M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-female",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-entrance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-public",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "parallel-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def inner_sum(m, j):\n",
    "    return (words[m] == j) @ ϕ[m, :N[m], :]\n",
    "\n",
    "def minorize(words, smooth=True):\n",
    "    \"\"\"\n",
    "    Minorize the joint likelihood function via variational inference.\n",
    "    This is the E-step of variational EM algorithm for (smoothed) LDA.\n",
    "    \"\"\"\n",
    "    global ϕ, γ\n",
    "    if smooth:\n",
    "        global λ\n",
    "    else:\n",
    "        global β\n",
    "    \n",
    "    # optimize ϕ\n",
    "    for m in tqdm(range(M)):\n",
    "        if smooth:\n",
    "            ϕ[m, :N[m], :] = np.exp(digamma(λ[:, words[m]]) - digamma(λ[:, :].sum(axis=1)).reshape(-1, 1) \\\n",
    "                                + (digamma(γ[m, :]) - digamma(γ[m, :].sum())).reshape(-1, 1)).T\n",
    "        else:\n",
    "            ϕ[m, :N[m], :] = (β[:, words[m]] * np.exp(digamma(γ[m, :]) - digamma(γ[m, :].sum())).reshape(-1, 1)).T\n",
    "                    \n",
    "        # Normalize phi\n",
    "        ϕ[m, :N[m], :] / ϕ[m, :N[m], :].sum(axis=1).reshape(-1, 1)\n",
    "    \n",
    "    # optimize γ\n",
    "    γ = α + np.array(list(map(lambda x: x.sum(axis=0), ϕ)))\n",
    "    \n",
    "    # optimize λ\n",
    "    if smooth:\n",
    "        λ[:] = η\n",
    "        for m in tqdm(range(M)):\n",
    "            λ += (np.array([words[m] == j for j in range(V)]) @ ϕ[m, :N[m], :]).T\n",
    "    \n",
    "        return φ, γ, λ\n",
    "    else:\n",
    "        return ϕ, γ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-occasion",
   "metadata": {},
   "source": [
    "#### M-step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-swing",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta_{ij} \\propto \\sum_{d=1}^M \\sum_{n=1}^N \\phi_{dni} \\mathbf{w}_{dn}^j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-flood",
   "metadata": {},
   "source": [
    "$\\alpha$ is updated via Newton-Raphson method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-friday",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\alpha_i} \n",
    "  = M\\left( \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right) - \\Psi(\\alpha_i) \\right)\n",
    "    - \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi\\left(\\sum_{j=1}^k \\gamma_{dj}\\right) \\right) \\\\\n",
    "\\frac{\\partial^2 L}{\\partial \\alpha_i \\alpha_j} = M \\left( \\Psi'\\left(\\sum_{j=1}^k \\alpha_j\\right) - \\delta(i,j) \\Psi'(\\alpha_i) \\right)\n",
    "$$\n",
    "\n",
    "where $\\delta(i,j) = 1$ if $i=j$, $0$ otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "advised-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def update_(var, vi_var, max_iter, tol):\n",
    "    \"\"\"\n",
    "    From appendix A.2 of Blei et al., 2003.\n",
    "    For hessian with shape `H = diag(h) + 1z1'`\n",
    "    \n",
    "    To update alpha, input var=alpha and vi_var=gamma.\n",
    "    To update eta, input var=eta and vi_var=lambda.\n",
    "    \"\"\"\n",
    "    for _ in tqdm(range(max_iter)):\n",
    "        var_old = var\n",
    "        \n",
    "        # g: gradient \n",
    "        digamma_sum = digamma(vi_var.sum(axis=1)).reshape(-1, 1)\n",
    "        g = M * (digamma(var.sum()) - digamma(var)) \\\n",
    "            + (digamma(vi_var) - digamma_sum).sum(axis=0)\n",
    "        \n",
    "        # H = diag(h) + 1z1'\n",
    "        h = -M * polygamma(1, var)       # h: Hessian diagonal component\n",
    "        z = M * polygamma(1, var.sum())  # z: Hessian constant component\n",
    "        c = (g / h).sum() / (1./z + (1./h).sum())\n",
    "\n",
    "        #  Update alpha\n",
    "        var = var - (g - c) / h\n",
    "        \n",
    "        #  Check convergence\n",
    "        if np.mean((var - var_old) ** 2) < tol ** 2:\n",
    "            break\n",
    "    \n",
    "    warnings.warn(f\"max_iter={max_iter} reached: values might not be optimal.\")\n",
    "    \n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "selected-specification",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximize(words, max_iter=1000, tol=1e-3, smooth=True):\n",
    "    \"\"\"\n",
    "    maximize the lower bound of the likelihood.\n",
    "    This is the M-step of variational EM algorithm for (smoothed) LDA.\n",
    "    \n",
    "    update of α follows from appendix A.2 of Blei et al., 2003.\n",
    "    \"\"\"\n",
    "    global α, γ\n",
    "    if smooth:\n",
    "        global η, λ\n",
    "    else:\n",
    "        global β, ϕ\n",
    "    \n",
    "    # update α\n",
    "    α = update_(α, γ, max_iter, tol)\n",
    "    \n",
    "    if smooth:\n",
    "        # update η\n",
    "        η = update_(η, λ, max_iter, tol)\n",
    "        \n",
    "        return α, η\n",
    "    \n",
    "    else:\n",
    "        # update β\n",
    "        for j in range(V):\n",
    "            β[:, j] = np.array([inner_sum(m, j) for m in range(M)]).sum(axis=1)\n",
    "        β /= β.sum(axis=1)\n",
    "    \n",
    "        return α, β"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-happening",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "extended-relief",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 171/7769 [00:00<00:08, 869.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      " MINORIZE =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7769/7769 [00:07<00:00, 1038.35it/s]\n",
      "  7%|▋         | 551/7769 [00:39<08:42, 13.81it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-212-d015b85a78a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MINORIZE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mφ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mγ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mλ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MAXIMIZE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-208-a25a166ecdba>\u001b[0m in \u001b[0;36mminorize\u001b[0;34m(words, smooth)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mλ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mη\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mλ\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mϕ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mφ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mγ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mλ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-208-a25a166ecdba>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mλ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mη\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mλ\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mϕ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mφ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mγ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mλ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MAX_ITER = 2\n",
    "for i in range(MAX_ITER):\n",
    "    print(i)\n",
    "    print(\"\"*5, \"MINORIZE\", \"=\"*5)\n",
    "    φ, γ, λ = minorize(words)\n",
    "    \n",
    "    print(\"=\"*5, \"MAXIMIZE\", \"=\"*5)\n",
    "    α, η = maximize(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-wilson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-table",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-worth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-broad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "3-1. Latent Dirichlet Allocation",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
