{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lesser-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-possibility",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>3-1. Latent Dirichlet Allocation<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data:-Reuters\" data-toc-modified-id=\"Data:-Reuters-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data: Reuters</a></span></li><li><span><a href=\"#Model:-Basic-LDA\" data-toc-modified-id=\"Model:-Basic-LDA-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Model: Basic LDA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Training</a></span></li></ul></li><li><span><a href=\"#Model:-Smoothed-LDA\" data-toc-modified-id=\"Model:-Smoothed-LDA-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Model: Smoothed LDA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Collaped-Gibbs-Sampling\" data-toc-modified-id=\"Collaped-Gibbs-Sampling-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Collaped Gibbs Sampling</a></span></li><li><span><a href=\"#Variational-EM\" data-toc-modified-id=\"Variational-EM-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Variational EM</a></span><ul class=\"toc-item\"><li><span><a href=\"#E-step\" data-toc-modified-id=\"E-step-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>E-step</a></span></li><li><span><a href=\"#M-step\" data-toc-modified-id=\"M-step-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>M-step</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-vertical",
   "metadata": {},
   "source": [
    "## Data: Reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-hungary",
   "metadata": {},
   "source": [
    "Reuters is a multi-class, multi-label dataset.\n",
    "\n",
    "* 90 classes\n",
    "* 10788 documents\n",
    "    * 7769 training documents\n",
    "    * 3019 testing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "green-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-liberty",
   "metadata": {},
   "source": [
    "* train-test split\n",
    ": The data is already splitted. Just sort it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ordered-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stopwords.words(\"english\")\n",
    "stops += [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "featured-tiger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "excellent-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = [], []\n",
    "vocab = []\n",
    "for file_id in reuters.fileids():\n",
    "    if file_id.startswith(\"train\"):\n",
    "        trainset.append([w.lower() for w in reuters.words(file_id) if w.lower() not in stops])\n",
    "        vocab += trainset[-1]\n",
    "    else:\n",
    "        testset.append([w.lower() for w in reuters.words(file_id) if w.lower() not in stops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "everyday-favorite",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(vocab))\n",
    "word_to_ix = {w: i for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "opened-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_ix(seq, vocab=vocab):\n",
    "    # len(vocab), which is the last index, is for the <unk> (unknown) token\n",
    "    unk_idx = len(vocab)\n",
    "    return torch.tensor(list(map(lambda w: word_to_ix.get(w, unk_idx), seq)))\n",
    "\n",
    "data = {\n",
    "    \"train\": list(map(seq_to_ix, trainset)),\n",
    "    \"test\": list(map(seq_to_ix, testset))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "tender-polish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20903, 16228, 13181,  3299,  3881])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][0][:5]  # word indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-recipe",
   "metadata": {},
   "source": [
    "## Model: Basic LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-evaluation",
   "metadata": {},
   "source": [
    "For each document $\\mathbf{w}$ in a corpus $D$, generate\n",
    "\n",
    "$$\n",
    "N \\sim \\mathcal{P}(\\xi) \\\\\n",
    "\\theta \\sim \\text{Dir}(\\alpha)\n",
    "$$\n",
    "\n",
    "and for $n = 1, \\cdots, N$, generate\n",
    "\n",
    "$$\n",
    "z_n \\sim \\text{Multi}(\\theta) \\\\\n",
    "w_n \\sim P(w_n | z_n, \\beta)\n",
    "$$\n",
    "\n",
    "where $\\beta \\in \\mathbb{R}^{k \\times V}$, $\\beta_{ij} = P(w^j = 1| z^i = 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-particular",
   "metadata": {},
   "source": [
    "* $\\alpha, \\beta$: hyperparameters (Dirichlet, Multinomial).\n",
    "* $N$: The number of words in the document. (ancillary variable)\n",
    "* $\\theta$: A topic mixture.\n",
    "* (For $i$ in $1\\cdots N$)\n",
    "  * $z_n$: A topic variable.\n",
    "  * $w_n$: A generated word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "continued-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lda(docs, n_topic, random_state=0):\n",
    "    global V, k, N, M, alpha, beta, gamma, phi\n",
    "    torch.manual_seed(random_state)\n",
    "    \n",
    "    V = len(vocab)\n",
    "    k = n_topic  # number of topics\n",
    "    N = torch.tensor([doc.shape[0] for doc in docs])\n",
    "    M = len(docs)\n",
    "\n",
    "    print(f\"V: {V}\\nk: {k}\\nN: {N[:10]}...\\nM: {M}\")\n",
    "\n",
    "    # initialize α, β\n",
    "    alpha = torch.rand(k)\n",
    "    beta = torch.ones((k, V)) / V\n",
    "\n",
    "    print(f\"α: dim {alpha.shape}\\nβ: dim {beta.shape}\")\n",
    "\n",
    "    # initialize ϕ, γ\n",
    "    ## ϕ: (M x max(N) x k) arrays with zero paddings on the right\n",
    "    gamma = alpha + torch.ones((M, k)) * N.reshape(-1, 1) / k\n",
    "\n",
    "    phi = torch.ones((M, max(N), k)) / k\n",
    "    for m, N_d in enumerate(N):\n",
    "        phi[m, N_d:, :] = 0  # zero padding for vectorized operations\n",
    "\n",
    "    print(f\"γ: dim {gamma.shape}\\nϕ: dim ({len(phi)}, N_d, {phi[0].shape[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "legal-alliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def E_step(docs, phi, gamma, alpha, beta):\n",
    "    \"\"\"\n",
    "    Minorize the joint likelihood function via variational inference.\n",
    "    This is the E-step of variational EM algorithm for (smoothed) LDA.\n",
    "    \"\"\"\n",
    "    # optimize phi\n",
    "    for m in range(M):\n",
    "        phi[m, :N[m], :] = (beta[:, docs[m]] * (\n",
    "            torch.exp(torch.digamma(gamma[m, :]) - torch.digamma(gamma[m, :].sum())).reshape(-1, 1))\n",
    "        ).T\n",
    "\n",
    "        # Normalize phi\n",
    "        phi[m, :N[m]] /= phi[m, :N[m]].sum(axis=1).reshape(-1, 1)\n",
    "        if torch.any(torch.isnan(phi)):\n",
    "            raise ValueError(\"phi nan\")\n",
    "\n",
    "    # optimize gamma\n",
    "    gamma = alpha + phi.sum(dim=1)\n",
    "\n",
    "    return phi, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "broadband-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def _update(var, vi_var, const, max_iter=10000, tol=1e-4):\n",
    "    \"\"\"\n",
    "    From appendix A.2 of Blei et al., 2003.\n",
    "    For hessian with shape `H = diag(h) + 1z1'`\n",
    "    \n",
    "    To update alpha, input var=alpha and vi_var=gamma, const=M.\n",
    "    To update eta, input var=eta and vi_var=lambda, const=k.\n",
    "    \"\"\"\n",
    "    for _ in range(max_iter):\n",
    "        # store old value\n",
    "        var0 = var.detach().clone()\n",
    "        \n",
    "        # g: gradient \n",
    "        psi_sum = torch.digamma(vi_var.sum(dim=1)).reshape(-1, 1)\n",
    "        g = const * (torch.digamma(var.sum()) - torch.digamma(var)) \\\n",
    "            + (torch.digamma(vi_var) - psi_sum).sum(dim=0)\n",
    "\n",
    "        # H = diag(h) + 1z1'\n",
    "        z = const * torch.polygamma(1, var.sum())  # z: Hessian constant component\n",
    "        h = -const * torch.polygamma(1, var)       # h: Hessian diagonal component\n",
    "        c = (g / h).sum() / (1./z + (1./h).sum())\n",
    "\n",
    "        # update var\n",
    "        var -= (g - c) / h\n",
    "        \n",
    "        # check convergence\n",
    "        err = torch.mean((var - var0) ** 2)\n",
    "        crit = err < tol ** 2\n",
    "        if crit:\n",
    "            break\n",
    "    else:\n",
    "        warnings.warn(f\"max_iter={max_iter} reached: values might not be optimal.\")\n",
    "    \n",
    "    #print(err)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "terminal-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _inner_sum(docs, phi, m, j):\n",
    "    # doc = np.zeros(docs[m].shape[0] * V, dtype=int)\n",
    "    # doc[np.arange(0, docs[m].shape[0] * V, V) + docs[m]] = 1\n",
    "    # doc = doc.reshape(-1, V)\n",
    "    # lam += phi[m, :N[m], :].T @ doc\n",
    "    return (docs[m] == j).float() @ phi[m, :N[m], :]\n",
    "\n",
    "def M_step(docs, phi, gamma, alpha, beta, M):\n",
    "    \"\"\"\n",
    "    maximize the lower bound of the likelihood.\n",
    "    This is the M-step of variational EM algorithm for (smoothed) LDA.\n",
    "    \n",
    "    update of alpha follows from appendix A.2 of Blei et al., 2003.\n",
    "    \"\"\"\n",
    "    # update alpha\n",
    "    alpha = _update(alpha, gamma, M)\n",
    "    \n",
    "    # update beta\n",
    "    for j in range(V):\n",
    "        beta[:, j] = torch.stack([_inner_sum(docs, phi, m, j) for m in range(M)]).sum(dim=0)\n",
    "    beta /= beta.sum(dim=1).reshape(-1, 1)\n",
    "\n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "minute-fifth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dg(gamma, d, i):\n",
    "    \"\"\"\n",
    "    E[log θ_t] where θ_t ~ Dir(gamma)\n",
    "    \"\"\"\n",
    "    return torch.digamma(gamma[d, i]) - torch.digamma(torch.sum(gamma[d, :]))\n",
    "\n",
    "\n",
    "def dl(lam, i, w_n):\n",
    "    \"\"\"\n",
    "    E[log β_t] where β_t ~ Dir(lam)\n",
    "    \"\"\"\n",
    "    return torch.digamma(lam[i, w_n]) - torch.digamma(torch.sum(lam[i, :]))\n",
    "\n",
    "\n",
    "def vlb(docs, phi, gamma, alpha, beta, M, N, k):\n",
    "    a, b, c, _d = 0, 0, 0, 0\n",
    "    for d in range(M):\n",
    "        a += (\n",
    "            torch.lgamma(alpha.sum())\n",
    "            - torch.lgamma(alpha).sum()\n",
    "            + torch.tensor([(alpha[i] - 1) * dg(gamma, d, i) for i in range(k)]).sum()\n",
    "        )\n",
    "\n",
    "        _d += (\n",
    "            torch.lgamma(gamma[d, :].sum())\n",
    "            - torch.lgamma(gamma[d, :]).sum()\n",
    "            + torch.tensor([(gamma[d, i] - 1) * dg(gamma, d, i) for i in range(k)]).sum()\n",
    "        )\n",
    "\n",
    "        for n in range(N[d]):\n",
    "            w_n = int(docs[d][n])\n",
    "\n",
    "            b += torch.tensor([phi[d][n, i] * dg(gamma, d, i) for i in range(k)]).sum()\n",
    "            c += torch.tensor([phi[d][n, i] * torch.log(beta[i, w_n]) for i in range(k)]).sum()\n",
    "            _d += torch.tensor([phi[d][n, i] * torch.log(phi[d][n, i]) for i in range(k)]).sum()\n",
    "\n",
    "    return a + b + c - _d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-welcome",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-reception",
   "metadata": {},
   "source": [
    "Only on 100 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "atlantic-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data[\"train\"][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "satisfactory-kennedy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V: 26118\n",
      "k: 3\n",
      "N: tensor([442, 173,  96, 100,  74,  24, 141,  80, 155, 100])...\n",
      "M: 100\n",
      "α: dim torch.Size([3])\n",
      "β: dim torch.Size([3, 26118])\n",
      "γ: dim torch.Size([100, 3])\n",
      "ϕ: dim (100, N_d, 3)\n"
     ]
    }
   ],
   "source": [
    "init_lda(docs, n_topic=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "diagnostic-northwest",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 000:  variational lower bound: -60571.770\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-d1edd76ec676>\u001b[0m in \u001b[0;36mM_step\u001b[0;34m(docs, phi, gamma, alpha, beta, M)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# update beta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_inner_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mbeta\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-d1edd76ec676>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# update beta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_inner_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mbeta\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-d1edd76ec676>\u001b[0m in \u001b[0;36m_inner_sum\u001b[0;34m(docs, phi, m, j)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# doc = doc.reshape(-1, V)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# lam += phi[m, :N[m], :].T @ doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mM_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "N_EPOCH = 1000\n",
    "TOL = 5\n",
    "\n",
    "verbose = True\n",
    "lb = -float(\"inf\")\n",
    "\n",
    "with torch.\n",
    "for epoch in range(N_EPOCH): \n",
    "    # store old value\n",
    "    lb_old = lb\n",
    "    \n",
    "    # Variational EM\n",
    "    phi, gamma = E_step(docs, phi, gamma, alpha, beta)\n",
    "    alpha, beta = M_step(docs, phi, gamma, alpha, beta, M)\n",
    "    \n",
    "    # check anomaly\n",
    "    if torch.any(torch.isnan(alpha)):\n",
    "        print(\"NaN detected: terminating\")\n",
    "        break\n",
    "    \n",
    "    # check convergence\n",
    "    lb = vlb(docs, phi, gamma, alpha, beta, M, N, k)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{epoch: 04}:  variational lower bound: {lb: .3f}\")\n",
    "    \n",
    "    if abs(lb - lb_old) < TOL:\n",
    "        break\n",
    "else:\n",
    "    warnings.warn(f\"max_iter reached: values might not be optimal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-range",
   "metadata": {},
   "source": [
    "* Training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-parameter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-andrew",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-treaty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-latino",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-glucose",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "certain-pixel",
   "metadata": {},
   "source": [
    "## Model: Smoothed LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-necessity",
   "metadata": {},
   "source": [
    "For each document $\\mathbf{w}$ in a corpus $D$, generate\n",
    "\n",
    "$$\n",
    "N \\sim \\mathcal{P}(\\xi) \\\\\n",
    "\\beta \\sim \\text{Dir}(\\lambda) \\\\\n",
    "\\theta \\sim \\text{Dir}(\\alpha)\n",
    "$$\n",
    "\n",
    "and for $n = 1, \\cdots, N$, generate\n",
    "\n",
    "$$\n",
    "z_n \\sim \\text{Multi}(\\theta) \\\\\n",
    "w_n \\sim P(w_n | z_n, \\beta)\n",
    "$$\n",
    "\n",
    "where $\\beta \\in \\mathbb{R}^{k \\times V}$, $\\beta_{ij} = P(w^j = 1| z^i = 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-identity",
   "metadata": {},
   "source": [
    "* $\\alpha, \\eta$: Dirichlet hyperparameters.\n",
    "* $\\beta$: Unsmoothed multinomial hyperparameter.\n",
    "* $N$: The number of words in the document. (ancillary variable)\n",
    "* $\\theta$: A topic mixture.\n",
    "* (For $i$ in $1\\cdots N$)\n",
    "  * $z_n$: A topic variable.\n",
    "  * $w_n$: A generated word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-hospital",
   "metadata": {},
   "source": [
    "### Collaped Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-synthesis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-leonard",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-domain",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hindu-telescope",
   "metadata": {},
   "source": [
    "### Variational EM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-favor",
   "metadata": {},
   "source": [
    "#### E-step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-thing",
   "metadata": {},
   "source": [
    "Let $\\phi_d \\in \\mathbb{R}^{N \\times k}, \\gamma_d \\in \\mathbb{R}^k, \\lambda \\in \\mathbb{R}^{k \\times V}$ be variational parameters for $\\alpha, \\beta, \\eta$.  \n",
    "Suppose further that for $\\beta \\in \\mathbb{R}^{k \\times V}$, $\\beta_i^0 \\sim \\text{Dir}(\\lambda^0)$ where $\\lambda_i^0 = \\eta$ for all $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-digit",
   "metadata": {},
   "source": [
    "For a document $\\mathbf{w}_d$, $d = 1,\\cdots,M$,\n",
    "\n",
    "1. initialize $\\phi_{dni}^0 := 1/k$ for all $i,n$.\n",
    "2. initialize $\\gamma_{di} := \\alpha_i + N/k$ for all $i$.\n",
    "3. **repeat until** convergence\n",
    "    1. for $n=1$ to $N$\n",
    "        1. for $i=1$ to $k$\n",
    "            1. $\\phi_{dni}^{t+1} := \\exp\\left(\\Psi(\\lambda_{iw_{dn}}^t) - \\Psi(\\sum_{j=1}^V \\lambda_{ij}^t) + \\Psi(\\gamma_{di}^t) - \\Psi(\\sum_{j=1}^k \\gamma_{dj}^t)\\right)$\n",
    "            1. for $j=1$ to $V$\n",
    "                1. $\\lambda_{ij} = \\eta + \\sum_{d=1}^M \\sum_{n=1}^{N_d} \\phi_{dni} w_{dn}^j$\n",
    "        2. normalize $\\phi_{dn}^{t+1}$ to sum to 1\n",
    "    2. $\\gamma_d^{t+1} := \\alpha + \\sum_{n=1}^N \\phi_{dn}^{t+1}$\n",
    "    \n",
    "where $\\Psi$ is the first derivative of the $\\log\\Gamma$ function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-occasion",
   "metadata": {},
   "source": [
    "#### M-step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-swing",
   "metadata": {},
   "source": [
    "$$\n",
    "\\beta_{ij} \\propto \\sum_{d=1}^M \\sum_{n=1}^N \\phi_{dni} \\mathbf{w}_{dn}^j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-flood",
   "metadata": {},
   "source": [
    "$\\alpha$ is updated via Newton-Raphson method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-friday",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\alpha_i} \n",
    "  = M\\left( \\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right) - \\Psi(\\alpha_i) \\right)\n",
    "    - \\sum_{d=1}^M \\left( \\Psi(\\gamma_{di}) - \\Psi\\left(\\sum_{j=1}^k \\gamma_{dj}\\right) \\right) \\\\\n",
    "\\frac{\\partial^2 L}{\\partial \\alpha_i \\alpha_j} = M \\left( \\Psi'\\left(\\sum_{j=1}^k \\alpha_j\\right) - \\delta(i,j) \\Psi'(\\alpha_i) \\right)\n",
    "$$\n",
    "\n",
    "where $\\delta(i,j) = 1$ if $i=j$, $0$ otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "through-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothedLDA:\n",
    "    \n",
    "    def __init__(self, docs, vocab, k):\n",
    "        self.docs = docs\n",
    "        \n",
    "        self.V = len(vocab)\n",
    "        self.k = k  # number of topics\n",
    "        self.N = np.array([doc.shape[0] for doc in docs])\n",
    "        self.M = len(docs)\n",
    "        \n",
    "        V = self.V\n",
    "        N = self.N\n",
    "        M = self.M\n",
    "        \n",
    "        # initialize model parameters\n",
    "        ##self.beta = np.ones((k, V)) / V\n",
    "        self.alpha = np.random.gamma(100, 0.01, k)\n",
    "        self.eta = np.ones(V)\n",
    "\n",
    "        # initialize variational parameters\n",
    "        # ϕ: (M x max(N) x k) arrays with zero paddings on the right\n",
    "        self.phi = [np.ones((N[d], k)) / k for d in range(M)]\n",
    "        self.gamma = alpha + (N / k).reshape(-1, 1)\n",
    "        self.lam = np.random.gamma(shape=100, scale=0.01, size=(k, V))\n",
    "        \n",
    "        \n",
    "    def _update_phi(self):\n",
    "        \"\"\"\n",
    "        Update variational parameter phi\n",
    "        ϕ_{n, j} ∝ e^[ (Ψ(λ_j) - Ψ(Σλ_j)) + ( Ψ(γ_j) - Ψ(Σγ_j) ) ]\n",
    "        \"\"\"\n",
    "        M = self.M\n",
    "        N = self.N\n",
    "        k = self.k\n",
    "\n",
    "        phi = self.phi\n",
    "        gamma = self.gamma\n",
    "        docs = self.docs\n",
    "\n",
    "        for d in range(M):\n",
    "            for n in range(N[d]):\n",
    "                for i in range(k):\n",
    "                    w_n = int(docs[d][n])\n",
    "                    phi[d][n, i] = np.exp(dl(lam, i, w_n) + dg(gamma, d, i))\n",
    "\n",
    "                # Normalize over topics\n",
    "                phi[d][n, :] = phi[d][n, :] / np.sum(phi[d][n, :])\n",
    "                \n",
    "        return phi\n",
    "    \n",
    "    def _update_gamma(self):\n",
    "        \"\"\"\n",
    "        Update variational parameter gamma\n",
    "        γ_t = α_t + Σ_{n=1}^{N_d} ϕ_{t, n}\n",
    "        \"\"\"\n",
    "        M = self.M\n",
    "        phi = self.phi\n",
    "        alpha = self.alpha\n",
    "\n",
    "        gamma = alpha + np.array(\n",
    "            list(map(lambda x: x.sum(axis=0), phi))\n",
    "        )\n",
    "        \n",
    "        return gamma\n",
    "    \n",
    "    \n",
    "    def _update_lam(self):\n",
    "        V = self.V\n",
    "        N = self.N\n",
    "        M = self.M\n",
    "        phi = self.phi\n",
    "        lam = self.lam\n",
    "        eta = self.eta\n",
    "        docs = self.docs\n",
    "        \n",
    "        lam[:] = eta\n",
    "        for d in range(M):  #, desc=\"MINORIZE lam\"):\n",
    "            doc = np.zeros(N[d] * V, dtype=int)\n",
    "            doc[np.arange(0, N[d] * V, V) + docs[d]] = 1\n",
    "            doc = doc.reshape(-1, V)\n",
    "            \n",
    "            lam += phi[d].T @ doc\n",
    "        \n",
    "        return lam\n",
    "        \n",
    "    \n",
    "    def _update_alpha(self, max_iter=1000, tol=0.1):\n",
    "        \"\"\"\n",
    "        Update alpha with linear time Newton-Raphson.\n",
    "        \"\"\"\n",
    "        M = self.M\n",
    "        k = self.k\n",
    "\n",
    "        alpha = self.alpha\n",
    "        gamma = self.gamma\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            alpha_old = alpha\n",
    "\n",
    "            #  Calculate gradient\n",
    "            g = M * (psi(np.sum(alpha)) - psi(alpha)) +\\\n",
    "                (psi(gamma) - psi(np.sum(gamma, axis=1)).reshape(-1, 1)).sum(axis=0)\n",
    "\n",
    "            #  Calculate Hessian diagonal component\n",
    "            h = -M * polygamma(1, alpha)\n",
    "\n",
    "            #  Calculate Hessian constant component\n",
    "            z = M * polygamma(1, np.sum(alpha))\n",
    "\n",
    "            #  Calculate constant\n",
    "            c = np.sum(g / h) / (z ** (-1.0) + np.sum(h ** (-1.0)))\n",
    "\n",
    "            #  Update alpha\n",
    "            alpha = alpha - (g - c) / h\n",
    "            \n",
    "            #  Check convergence\n",
    "            if np.sqrt(np.mean(np.square(alpha - alpha_old))) < tol:\n",
    "                break\n",
    "        else:\n",
    "            warnings.warn(\"_update_alpha(): max_iter reached.\")\n",
    "\n",
    "        return alpha\n",
    "    \n",
    "    \n",
    "    def _update_eta(self, max_iter=1000, tol=0.1):\n",
    "        \"\"\"\n",
    "        Update eta with linear time Newton-Raphson.\n",
    "        \"\"\"\n",
    "        M = self.M\n",
    "        k = self.k\n",
    "\n",
    "        eta = self.eta\n",
    "        lam = self.lam\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            eta_old = eta\n",
    "\n",
    "            #  Calculate gradient\n",
    "            g = k * (psi(np.sum(eta)) - psi(eta)) +\\\n",
    "                (psi(lam) - psi(np.sum(lam, axis=1)).reshape(-1, 1)).sum(axis=0)\n",
    "\n",
    "            #  Calculate Hessian diagonal component\n",
    "            h = -k * polygamma(1, eta)\n",
    "\n",
    "            #  Calculate Hessian constant component\n",
    "            z = k * polygamma(1, np.sum(eta))\n",
    "\n",
    "            #  Calculate constant\n",
    "            c = np.sum(g / h) / (z ** (-1.0) + np.sum(h ** (-1.0)))\n",
    "\n",
    "            #  Update alpha\n",
    "            eta = eta - (g - c) / h\n",
    "\n",
    "            #  Check convergence\n",
    "            if np.sqrt(np.mean(np.square(eta - eta_old))) < tol:\n",
    "                break\n",
    "        else:\n",
    "            warnings.warn(\"_update_eta(): max_iter reached.\")\n",
    "\n",
    "        return eta\n",
    "    \n",
    "    \n",
    "    def _E_step(self):\n",
    "        \"\"\"\n",
    "        E-step of the variational EM algorithm.\n",
    "        Update ϕ, γ, λ.\n",
    "        \"\"\"\n",
    "        self.phi = self._update_phi()\n",
    "        self.gamma = self._update_gamma()\n",
    "        self.lam = self._update_lam()\n",
    "        \n",
    "        \n",
    "    def _M_step(self):\n",
    "        \"\"\"\n",
    "        M-step of the variational EM algorithm.\n",
    "        Update α, η.\n",
    "        \"\"\"\n",
    "        self.alpha = self._update_alpha()\n",
    "        self.eta = self._update_eta()\n",
    "    \n",
    "    \n",
    "    def vlb(self):\n",
    "        \"\"\"\n",
    "        lower bound from variational inference\n",
    "        \"\"\"\n",
    "        phi = self.phi\n",
    "        gamma = self.gamma\n",
    "        lam = self.lam\n",
    "        alpha = self.alpha\n",
    "        eta = self.eta\n",
    "        docs = self.docs\n",
    "        \n",
    "        M = self.M\n",
    "        k = self.k\n",
    "        N = self.N\n",
    "        \n",
    "        a0, a1, a2, a3_1, a3_2, a4, a5 = 0., 0., 0., 0., 0., 0., 0.\n",
    "        for d in range(M):\n",
    "            a0 += (\n",
    "                k * (\n",
    "                    gammaln(np.sum(eta)) \n",
    "                    - np.sum(gammaln(eta))\n",
    "                )\n",
    "                + np.sum([(eta[j] - 1) * dl(lam, i, j) for j in range(V) for i in range(k)])\n",
    "            )\n",
    "            a1 += (\n",
    "                gammaln(np.sum(alpha))\n",
    "                - np.sum(gammaln(alpha))\n",
    "                + np.sum([(alpha[i] - 1) * dg(gamma, d, i) for i in range(k)])\n",
    "            )\n",
    "\n",
    "            a4 += (\n",
    "                gammaln(np.sum(gamma[d, :]))\n",
    "                - np.sum(gammaln(gamma[d, :]))\n",
    "                + np.sum([(gamma[d, i] - 1) * dg(gamma, d, i) for i in range(k)])\n",
    "            )\n",
    "            \n",
    "            for i in range(k):\n",
    "                for j in range(V):\n",
    "                    a3_2 += (\n",
    "                        gammaln(np.sum(lam[i, j]))\n",
    "                        - np.sum(gammaln(lam[i, :]))\n",
    "                        + np.sum((lam[i, j] - 1) * dl(lam, i, j))\n",
    "                    )\n",
    "\n",
    "            for n in range(N[d]):\n",
    "                w_n = int(docs[d][n])\n",
    "                a2 += np.sum([phi[d][n, i] * dg(gamma, d, i) for i in range(k)])\n",
    "                a3_1 += np.sum([phi[d][n, i] * dl(lam, i, w_n) for i in range(k)])\n",
    "                a5 += np.sum([phi[d][n, i] * np.log(phi[d][n, i]) for i in range(k)])\n",
    "\n",
    "        return a0 + a1 + a2 + a3_1 - a3_2 - a4 - a5\n",
    "    \n",
    "    \n",
    "    def train(self, max_iter=1000, tol=5, verbose=True):\n",
    "        vlb = -np.inf\n",
    "        \n",
    "        for it in range(max_iter):\n",
    "            old_vlb = vlb\n",
    "            self._E_step()\n",
    "            self._M_step()\n",
    "            \n",
    "            vlb = self.vlb()\n",
    "            err = vlb - old_vlb\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Iteration {it+1}: {vlb: .3f} (delta: {err: .2f})\") \n",
    "            \n",
    "            if err < tol:\n",
    "                break\n",
    "        else:\n",
    "            warnings.warn(\"max_iter reached.\")\n",
    "            \n",
    "\n",
    "    \n",
    "def dg(gamma, d, i):\n",
    "    \"\"\"\n",
    "    E[log θ_t] where θ_t ~ Dir(gamma)\n",
    "    \"\"\"\n",
    "    return psi(gamma[d, i]) - psi(np.sum(gamma[d, :]))\n",
    "\n",
    "\n",
    "def dl(lam, i, w_n):\n",
    "    \"\"\"\n",
    "    E[log β_t] where β_t ~ Dir(lam)\n",
    "    \"\"\"\n",
    "    return psi(lam[i, w_n]) - psi(np.sum(lam[i, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-tourism",
   "metadata": {},
   "source": [
    "```python\n",
    "lda = SmoothedLDA(docs, vocab, k)\n",
    "lda.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-publication",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-approval",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-document",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_x86",
   "language": "python",
   "name": "pytorch_x86"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "3-1. Latent Dirichlet Allocation",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
